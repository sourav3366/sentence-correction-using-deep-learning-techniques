{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8efnXnyt2CkG"
   },
   "source": [
    "## Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "RuDH_s5h0SOU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLs_222j2Ikt"
   },
   "source": [
    "## Function -1\n",
    "#### For generating prediction value on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wHyLzaQI2FOx"
   },
   "outputs": [],
   "source": [
    "def generating_prediction(train,input_sentence,Original_Sentence): \n",
    "    if input_sentence.endswith('>'):\n",
    "        input_sentence = input_sentence[:-1]\n",
    "    if Original_Sentence.endswith('>'):\n",
    "        Original_Sentence = Original_Sentence[:-1]       \n",
    "    train.iloc[0]['informal_text_inp']= str(train.iloc[0]['informal_text_inp'])+'>'\n",
    "    train.iloc[0]['normal_text_inp']= str(train.iloc[0]['normal_text_inp'])+'>'\n",
    "    tknizer_informal = Tokenizer(filters = '\"#$%&()*+-/=@[\\\\]^_`{|}~\\t\\n', lower = False, char_level = True)\n",
    "    tknizer_informal.fit_on_texts(train['informal_text_inp'].values)\n",
    "    tknizer_normal = Tokenizer(filters = '\"#$%&()*+-/=@[\\\\]^_`{|}~\\t\\n', lower = False, char_level = True)\n",
    "    tknizer_normal.fit_on_texts(train['normal_text_inp'].values)\n",
    "\n",
    "    vocab_size_informal=len(tknizer_informal.word_index.keys())\n",
    "    vocab_size_normal=len(tknizer_normal.word_index.keys())\n",
    "    UNITS = 200\n",
    "    MAX_LEN = 200\n",
    "    model = tf.keras.models.load_model('Model_concat', custom_objects={\"loss_function\": loss_function})\n",
    "\n",
    "    inputs = [tknizer_informal.word_index.get(i, 0) for i in input_sentence]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen = MAX_LEN, padding = 'post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = tf.zeros([1, UNITS]), tf.zeros([1, UNITS])\n",
    "    enc_out, state_h, state_c = model.encoder([inputs, hidden])\n",
    "    dec_hidden = [state_h, state_c]\n",
    "    dec_input = tf.expand_dims([tknizer_normal.word_index['<']], 0)\n",
    "    for t in range(MAX_LEN):\n",
    "        output, state_h, state_c = model.decoder.timestepdecoder([dec_input, enc_out, state_h, state_c])\n",
    "        predicted_id = tf.argmax(output[0]).numpy()\n",
    "        if tknizer_normal.index_word.get(predicted_id, '') == '>':\n",
    "            break\n",
    "        else:\n",
    "            result += tknizer_normal.index_word.get(predicted_id, '')\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result,Original_Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b51gYZbw2gc4",
    "outputId": "617309a1-589a-40fc-8777-e845dea6718a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  Sentence is    :  <Watche Just Married. Haha.>\n",
      "Predicted sentence is :  Watch Just Marail.\n",
      "Original Sentence is  :  Watch Just Married. Haha.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "df=pd.read_csv('/content/with_augmentation_2.csv')\n",
    "train, validation = train_test_split(df, test_size=0.05, random_state = 859)\n",
    "train, test = train_test_split(train, test_size=0.05, random_state = 859)\n",
    "data_for_prediction = test['informal_text_inp'].iloc[0]\n",
    "Original_Sentence   = test['normal_text_out'].iloc[0]\n",
    "pred,Original=generating_prediction(train,data_for_prediction,Original_Sentence)\n",
    "print('Input  Sentence is    : ',data_for_prediction)\n",
    "print('Predicted sentence is : ',pred)\n",
    "print('Original Sentence is  : ',Original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HipDvdNtFNac"
   },
   "source": [
    "## Function 2\n",
    "#### For getting BLEU SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "J3TMpzLLCgXT"
   },
   "outputs": [],
   "source": [
    "def generating_prediction(train,input_sentence,Original_Sentence): \n",
    "    if input_sentence.endswith('>'):\n",
    "        input_sentence = input_sentence[:-1]\n",
    "    if Original_Sentence.endswith('>'):\n",
    "        Original_Sentence = Original_Sentence[:-1]       \n",
    "    train.iloc[0]['informal_text_inp']= str(train.iloc[0]['informal_text_inp'])+'>'\n",
    "    train.iloc[0]['normal_text_inp']= str(train.iloc[0]['normal_text_inp'])+'>'\n",
    "    tknizer_informal = Tokenizer(filters = '\"#$%&()*+-/=@[\\\\]^_`{|}~\\t\\n', lower = False, char_level = True)\n",
    "    tknizer_informal.fit_on_texts(train['informal_text_inp'].values)\n",
    "    tknizer_normal = Tokenizer(filters = '\"#$%&()*+-/=@[\\\\]^_`{|}~\\t\\n', lower = False, char_level = True)\n",
    "    tknizer_normal.fit_on_texts(train['normal_text_inp'].values)\n",
    "\n",
    "    vocab_size_informal=len(tknizer_informal.word_index.keys())\n",
    "    vocab_size_normal=len(tknizer_normal.word_index.keys())\n",
    "    UNITS = 200\n",
    "    MAX_LEN = 200\n",
    "    model = tf.keras.models.load_model('Model_concat', custom_objects={\"loss_function\": loss_function})\n",
    "\n",
    "    inputs = [tknizer_informal.word_index.get(i, 0) for i in input_sentence]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen = MAX_LEN, padding = 'post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = tf.zeros([1, UNITS]), tf.zeros([1, UNITS])\n",
    "    enc_out, state_h, state_c = model.encoder([inputs, hidden])\n",
    "    dec_hidden = [state_h, state_c]\n",
    "    dec_input = tf.expand_dims([tknizer_normal.word_index['<']], 0)\n",
    "    for t in range(MAX_LEN):\n",
    "        output, state_h, state_c = model.decoder.timestepdecoder([dec_input, enc_out, state_h, state_c])\n",
    "        predicted_id = tf.argmax(output[0]).numpy()\n",
    "        if tknizer_normal.index_word.get(predicted_id, '') == '>':\n",
    "            break\n",
    "        else:\n",
    "            result += tknizer_normal.index_word.get(predicted_id, '')\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    result=result.split()       \n",
    "    Original_Sentence=Original_Sentence.split() \n",
    "    score=bleu.sentence_bleu([Original_Sentence], result)      \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bk2Sel-IDBeL",
    "outputId": "1d2a363d-2189-43f5-a376-ed33d06fcfc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score of predicted sentence is:  0.5444460596606694\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk.translate.bleu_score as bleu\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "df=pd.read_csv('/content/with_augmentation_2.csv')\n",
    "train, validation = train_test_split(df, test_size=0.05, random_state = 859)\n",
    "train, test = train_test_split(train, test_size=0.05, random_state = 859)\n",
    "data_for_prediction = test['informal_text_inp'].iloc[0]\n",
    "Original_Sentence   = test['normal_text_out'].iloc[0]\n",
    "score=generating_prediction(train,data_for_prediction,Original_Sentence)\n",
    "print('Bleu score of predicted sentence is: ',score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
